"""Static analysis module for suspicious file examination.

Performs non-execution analysis of files: magic byte identification, hash computation,
string extraction, entropy analysis, PE header inspection, and suspicious indicator detection.
Defensive tool for malware triage and initial assessment.
"""

import asyncio
import hashlib
import math
import os
import re
import struct
from typing import Any, Dict, List, Tuple

from src.core.base_module import (
    AtsModule,
    ModuleSpec,
    ModuleCategory,
    Parameter,
    ParameterType,
    OutputField,
)

# Magic bytes for common file types
MAGIC_SIGNATURES: List[Tuple[bytes, str]] = [
    (b"MZ", "PE Executable (EXE/DLL)"),
    (b"\x7fELF", "ELF Binary (Linux)"),
    (b"\xfe\xed\xfa\xce", "Mach-O Binary (32-bit)"),
    (b"\xfe\xed\xfa\xcf", "Mach-O Binary (64-bit)"),
    (b"\xca\xfe\xba\xbe", "Mach-O Universal Binary / Java Class"),
    (b"PK\x03\x04", "ZIP Archive (or DOCX/XLSX/JAR/APK)"),
    (b"\x1f\x8b", "GZIP Archive"),
    (b"Rar!\x1a\x07", "RAR Archive"),
    (b"\x25PDF", "PDF Document"),
    (b"\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1", "OLE2 Compound (DOC/XLS/PPT)"),
    (b"#!/", "Shell Script"),
    (b"\x89PNG", "PNG Image"),
    (b"\xff\xd8\xff", "JPEG Image"),
    (b"GIF8", "GIF Image"),
    (b"BM", "BMP Image"),
    (b"7z\xbc\xaf\x27\x1c", "7-Zip Archive"),
    (b"\x00\x00\x01\x00", "ICO Icon"),
]

# Suspicious Windows API calls commonly used by malware
SUSPICIOUS_IMPORTS = [
    "VirtualAlloc", "VirtualProtect", "WriteProcessMemory", "CreateRemoteThread",
    "NtUnmapViewOfSection", "SetWindowsHookEx", "GetAsyncKeyState", "GetKeyState",
    "InternetOpen", "URLDownloadToFile", "WinExec", "ShellExecute",
    "CreateProcess", "OpenProcess", "ReadProcessMemory", "LoadLibrary",
    "GetProcAddress", "RegSetValue", "RegCreateKey", "CryptEncrypt",
    "CryptDecrypt", "IsDebuggerPresent", "CheckRemoteDebuggerPresent",
    "NtQueryInformationProcess", "GetTickCount", "Sleep",
    "FindFirstFile", "FindNextFile", "DeleteFile", "MoveFile",
    "CreateFile", "WriteFile", "CreateService", "StartService",
    "AdjustTokenPrivileges", "LookupPrivilegeValue",
]

READ_CHUNK = 65536


class StaticAnalyzerModule(AtsModule):
    """Static analysis of suspicious files for malware triage."""

    def get_spec(self) -> ModuleSpec:
        return ModuleSpec(
            name="static_analyzer",
            category=ModuleCategory.MALWARE,
            description="Static analysis of suspicious files: magic bytes, hashes, strings, entropy, PE headers, and suspicious indicators",
            version="1.0.0",
            parameters=[
                Parameter(
                    name="file_path",
                    type=ParameterType.FILE,
                    description="Path to the suspicious file to analyze",
                    required=True,
                ),
                Parameter(
                    name="analysis_depth",
                    type=ParameterType.CHOICE,
                    description="Depth of analysis to perform",
                    required=False,
                    default="standard",
                    choices=["quick", "standard", "deep"],
                ),
            ],
            outputs=[
                OutputField(name="file_info", type="dict", description="File metadata and type identification"),
                OutputField(name="hashes", type="dict", description="MD5 and SHA256 hashes of the file"),
                OutputField(name="strings_found", type="dict", description="Categorized extracted strings"),
                OutputField(name="entropy_analysis", type="dict", description="Shannon entropy per section and overall"),
                OutputField(name="suspicious_indicators", type="list", description="Detected suspicious patterns and indicators"),
            ],
            tags=["malware", "static-analysis", "triage", "hashing", "entropy"],
            author="ATS-Toolkit",
            dangerous=True,
        )

    def validate_inputs(self, config: Dict[str, Any]) -> Tuple[bool, str]:
        file_path = config.get("file_path", "").strip()
        if not file_path:
            return False, "file_path is required"
        if not os.path.isfile(file_path):
            return False, f"File not found: {file_path}"
        return True, ""

    def _identify_file_type(self, data: bytes) -> str:
        """Identify file type from magic bytes."""
        for magic, description in MAGIC_SIGNATURES:
            if data[:len(magic)] == magic:
                return description
        # Check for plain text / script
        try:
            data[:512].decode("utf-8")
            return "Text/Script file"
        except UnicodeDecodeError:
            return "Unknown binary"

    def _compute_hashes(self, file_path: str) -> Dict[str, str]:
        """Compute MD5 and SHA256 hashes."""
        md5 = hashlib.md5()
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as fh:
            while True:
                chunk = fh.read(READ_CHUNK)
                if not chunk:
                    break
                md5.update(chunk)
                sha256.update(chunk)
        return {"md5": md5.hexdigest(), "sha256": sha256.hexdigest()}

    def _shannon_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of a byte sequence."""
        if not data:
            return 0.0
        freq = [0] * 256
        for byte in data:
            freq[byte] += 1
        length = len(data)
        entropy = 0.0
        for count in freq:
            if count > 0:
                p = count / length
                entropy -= p * math.log2(p)
        return round(entropy, 4)

    def _extract_strings(self, data: bytes, min_len: int = 4) -> Dict[str, List[str]]:
        """Extract categorized strings from binary data."""
        # Extract printable ASCII strings
        ascii_pattern = re.compile(rb"[\x20-\x7e]{%d,}" % min_len)
        raw_strings = [m.group().decode("ascii", errors="ignore") for m in ascii_pattern.finditer(data)]

        # Extract UTF-16LE strings
        utf16_strings = []
        i = 0
        current = []
        while i < len(data) - 1:
            lo, hi = data[i], data[i + 1]
            if hi == 0 and 32 <= lo < 127:
                current.append(chr(lo))
                i += 2
            else:
                if len(current) >= min_len:
                    utf16_strings.append("".join(current))
                current = []
                i += 1
        if len(current) >= min_len:
            utf16_strings.append("".join(current))

        all_strings = list(set(raw_strings + utf16_strings))

        # Categorize
        url_re = re.compile(r"https?://[^\s\"'<>]{5,200}", re.IGNORECASE)
        ip_re = re.compile(r"\b(?:(?:25[0-5]|2[0-4]\d|1\d{2}|[1-9]?\d)\.){3}(?:25[0-5]|2[0-4]\d|1\d{2}|[1-9]?\d)\b")
        email_re = re.compile(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}")
        registry_re = re.compile(r"HKEY_[A-Z_]+\\[^\s\"]{5,}", re.IGNORECASE)

        categorized: Dict[str, List[str]] = {
            "urls": [], "ip_addresses": [], "emails": [], "registry_keys": [],
            "suspicious_apis": [], "notable_strings": [],
        }

        seen: set = set()
        for s in all_strings:
            for url in url_re.findall(s):
                if url not in seen:
                    seen.add(url)
                    categorized["urls"].append(url)
            for ip in ip_re.findall(s):
                if ip not in seen:
                    seen.add(ip)
                    categorized["ip_addresses"].append(ip)
            for em in email_re.findall(s):
                if em not in seen:
                    seen.add(em)
                    categorized["emails"].append(em)
            for rk in registry_re.findall(s):
                if rk not in seen:
                    seen.add(rk)
                    categorized["registry_keys"].append(rk)
            for api in SUSPICIOUS_IMPORTS:
                if api in s and api not in seen:
                    seen.add(api)
                    categorized["suspicious_apis"].append(api)

        categorized["total_ascii"] = len(raw_strings)
        categorized["total_utf16"] = len(utf16_strings)
        return categorized

    def _analyze_pe(self, data: bytes) -> Dict[str, Any]:
        """Basic PE header analysis if file is a PE executable."""
        info: Dict[str, Any] = {"is_pe": False}
        if data[:2] != b"MZ" or len(data) < 64:
            return info

        info["is_pe"] = True
        try:
            pe_offset = struct.unpack_from("<I", data, 0x3C)[0]
            if pe_offset + 6 > len(data) or data[pe_offset:pe_offset + 4] != b"PE\x00\x00":
                info["valid_pe_signature"] = False
                return info

            info["valid_pe_signature"] = True
            machine = struct.unpack_from("<H", data, pe_offset + 4)[0]
            machine_map = {0x14C: "x86 (i386)", 0x8664: "x86-64 (AMD64)", 0x1C0: "ARM", 0xAA64: "ARM64"}
            info["machine"] = machine_map.get(machine, f"Unknown (0x{machine:04X})")

            num_sections = struct.unpack_from("<H", data, pe_offset + 6)[0]
            info["number_of_sections"] = num_sections

            characteristics = struct.unpack_from("<H", data, pe_offset + 22)[0]
            info["is_dll"] = bool(characteristics & 0x2000)
            info["is_executable"] = bool(characteristics & 0x0002)

            # Optional header magic
            opt_offset = pe_offset + 24
            if opt_offset + 2 <= len(data):
                opt_magic = struct.unpack_from("<H", data, opt_offset)[0]
                info["pe_format"] = "PE32+" if opt_magic == 0x20B else "PE32"

            # Parse section table for entropy analysis
            opt_hdr_size = struct.unpack_from("<H", data, pe_offset + 20)[0]
            section_offset = pe_offset + 24 + opt_hdr_size
            sections = []
            for i in range(min(num_sections, 96)):
                s_off = section_offset + i * 40
                if s_off + 40 > len(data):
                    break
                name = data[s_off:s_off + 8].rstrip(b"\x00").decode("ascii", errors="replace")
                virtual_size = struct.unpack_from("<I", data, s_off + 8)[0]
                raw_size = struct.unpack_from("<I", data, s_off + 16)[0]
                raw_ptr = struct.unpack_from("<I", data, s_off + 20)[0]
                sec_chars = struct.unpack_from("<I", data, s_off + 36)[0]
                section_data = data[raw_ptr:raw_ptr + raw_size] if raw_ptr + raw_size <= len(data) else b""
                entropy = self._shannon_entropy(section_data)
                sections.append({
                    "name": name,
                    "virtual_size": virtual_size,
                    "raw_size": raw_size,
                    "entropy": entropy,
                    "executable": bool(sec_chars & 0x20000000),
                    "writable": bool(sec_chars & 0x80000000),
                })
            info["sections"] = sections
        except (struct.error, IndexError):
            info["parse_error"] = "Failed to fully parse PE headers"

        return info

    def _detect_indicators(self, data: bytes, pe_info: Dict, strings_info: Dict) -> List[Dict[str, str]]:
        """Detect suspicious indicators from all gathered data."""
        indicators: List[Dict[str, str]] = []

        # High entropy sections (possible packing/encryption)
        for section in pe_info.get("sections", []):
            if section["entropy"] > 7.0:
                indicators.append({
                    "type": "high_entropy_section",
                    "detail": f"Section '{section['name']}' has entropy {section['entropy']} (likely packed/encrypted)",
                    "severity": "high",
                })
            if section["writable"] and section["executable"]:
                indicators.append({
                    "type": "rwx_section",
                    "detail": f"Section '{section['name']}' is both writable and executable",
                    "severity": "medium",
                })

        # Suspicious API usage
        if strings_info.get("suspicious_apis"):
            indicators.append({
                "type": "suspicious_apis",
                "detail": f"Found {len(strings_info['suspicious_apis'])} suspicious API references: {', '.join(strings_info['suspicious_apis'][:10])}",
                "severity": "medium",
            })

        # Network indicators in binary
        if strings_info.get("urls"):
            indicators.append({
                "type": "embedded_urls",
                "detail": f"Found {len(strings_info['urls'])} embedded URLs",
                "severity": "medium",
            })
        if strings_info.get("ip_addresses"):
            indicators.append({
                "type": "embedded_ips",
                "detail": f"Found {len(strings_info['ip_addresses'])} embedded IP addresses",
                "severity": "medium",
            })

        # PE anomalies
        if pe_info.get("is_pe") and pe_info.get("number_of_sections", 0) > 10:
            indicators.append({
                "type": "many_sections",
                "detail": f"PE has {pe_info['number_of_sections']} sections (unusually high)",
                "severity": "low",
            })

        # Packer signatures in strings
        packer_sigs = ["UPX", "ASPack", "PECompact", "Themida", "VMProtect", "Enigma", "Armadillo"]
        found_packers = [p for p in packer_sigs if any(p.lower() in s.lower() for s in [sec.get("name", "") for sec in pe_info.get("sections", [])])]
        if found_packers:
            indicators.append({
                "type": "packer_detected",
                "detail": f"Possible packer signatures: {', '.join(found_packers)}",
                "severity": "high",
            })

        return indicators

    async def execute(self, config: Dict[str, Any]) -> Dict[str, Any]:
        file_path = config["file_path"].strip()
        depth = config.get("analysis_depth", "standard")

        self.logger.info("starting_static_analysis", file=file_path, depth=depth)

        loop = asyncio.get_event_loop()
        file_size = os.path.getsize(file_path)

        # Read the file
        def _read_file() -> bytes:
            with open(file_path, "rb") as fh:
                return fh.read()

        data = await loop.run_in_executor(None, _read_file)

        # File info
        file_type = self._identify_file_type(data)
        file_info = {
            "path": file_path,
            "name": os.path.basename(file_path),
            "size": file_size,
            "type": file_type,
        }

        # Hashes
        hashes = self._compute_hashes(file_path)

        # Overall entropy
        overall_entropy = self._shannon_entropy(data)
        entropy_analysis: Dict[str, Any] = {"overall": overall_entropy}
        if overall_entropy > 7.0:
            entropy_analysis["assessment"] = "Very high entropy - likely encrypted or compressed"
        elif overall_entropy > 6.0:
            entropy_analysis["assessment"] = "High entropy - possibly packed or contains compressed data"
        else:
            entropy_analysis["assessment"] = "Normal entropy range"

        # Strings extraction (min_len depends on depth)
        min_len = {"quick": 6, "standard": 4, "deep": 3}.get(depth, 4)
        strings_found = self._extract_strings(data, min_len)

        # PE analysis
        pe_info = self._analyze_pe(data)
        if pe_info.get("sections"):
            entropy_analysis["sections"] = [
                {"name": s["name"], "entropy": s["entropy"]} for s in pe_info["sections"]
            ]
        file_info["pe_info"] = pe_info if pe_info.get("is_pe") else None

        # Suspicious indicators
        suspicious_indicators = self._detect_indicators(data, pe_info, strings_found)

        self.logger.info(
            "static_analysis_complete",
            file_type=file_type,
            indicators=len(suspicious_indicators),
        )

        return {
            "file_info": file_info,
            "hashes": hashes,
            "strings_found": strings_found,
            "entropy_analysis": entropy_analysis,
            "suspicious_indicators": suspicious_indicators,
        }
